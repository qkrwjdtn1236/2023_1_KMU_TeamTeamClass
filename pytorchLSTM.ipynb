{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data_als\n",
    "import data_prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = data_prepro.loadData('datasets/data_tr_city.csv','datasets/data_ts_city.csv',trainYearRange1=2019,trainYearRange2=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([17558, 21595, 25626], dtype='int64')\n",
      "index                  0\n",
      "datetime               0\n",
      "구미 혁신도시배수지 유출유량 적산차    2\n",
      "year                   0\n",
      "month                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = data_prepro.outlierDataToNan(train,low=True,high=False,thres=10.0)\n",
    "# train = data_prepro.fillZero(train)\n",
    "train = data_prepro.fillnaBehind(train)\n",
    "train = data_prepro.fillprevValue(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = data_prepro.XDataToXAndYSeq(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test = data_prepro.XDataToXAndYSeq(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17519, 1, 24), (17519, 1))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1,1,24)\n",
    "y_train = y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17519, 1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(-1,1,24)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape torch.Size([17519, 1, 24]) torch.Size([17519, 1])\n",
      "Testing Shape torch.Size([8399, 1, 24]) torch.Size([8399, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensors = Variable(torch.Tensor(x_train))\n",
    "X_test_tensors = Variable(torch.Tensor(x_test))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test))\n",
    "\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors,   X_train_tensors.shape)\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors,  X_test_tensors.shape) \n",
    "\n",
    "print(\"Training Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
    "print(\"Testing Shape\", X_test_tensors_final.shape, y_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla M40 24GB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # device\n",
    "except:\n",
    "    device = \"cpu\"\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "    super(LSTM1, self).__init__()\n",
    "    self.num_classes = num_classes #number of classes\n",
    "    self.num_layers = num_layers #number of layers\n",
    "    self.input_size = input_size #input size\n",
    "    self.hidden_size = hidden_size #hidden state\n",
    "    self.seq_length = seq_length #sequence length\n",
    " \n",
    "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                      num_layers=num_layers, batch_first=True) #lstm\n",
    "    self.fc_1 =  nn.Linear(hidden_size, 1024) #fully connected 1\n",
    "    self.fc = nn.Linear(1024, num_classes) #fully connected last layer\n",
    "\n",
    "    self.relu = nn.ReLU() \n",
    "\n",
    "  def forward(self,x):\n",
    "    h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
    "    c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state   \n",
    "    # Propagate input through LSTM\n",
    "\n",
    "    output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "   \n",
    "    hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "    out = self.relu(hn)\n",
    "    out = self.fc_1(out) #first Dense\n",
    "    out = self.relu(out) #relu\n",
    "    out = self.fc(out) #Final Output\n",
    "   \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM1(nn.Module):\n",
    "#   def __init__(self, num_classes, input_size, hidden_size, num_layers:list, seq_length):\n",
    "#     super(LSTM1, self).__init__()\n",
    "#     self.num_classes = num_classes #number of classes\n",
    "#     self.num_layers = num_layers #number of layers\n",
    "#     self.input_size = input_size #input size\n",
    "#     self.hidden_size = hidden_size #hidden state\n",
    "#     self.seq_length = seq_length #sequence length\n",
    " \n",
    "#     self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size[0],\n",
    "#                       num_layers=num_layers, batch_first=True,bidirectional =True) #lstm\n",
    "#     self.lstm2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size[1],\n",
    "#                       num_layers=num_layers, batch_first=True,bidirectional =True) #lstm\n",
    "#     self.fc_1 =  nn.Linear(hidden_size[1]*2, 3072) #fully connected 1\n",
    "#     self.fc = nn.Linear(3072, num_classes) #fully connected last layer\n",
    "\n",
    "    \n",
    "#     self.relu = nn.ReLU() \n",
    "#     self.tanh = nn.Tanh()\n",
    "\n",
    "#   def forward(self,x):\n",
    "#     h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
    "#     c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state   \n",
    "#     # Propagate input through LSTM\n",
    "\n",
    "#     output, (hn, cn) = self.lstm1(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "\n",
    "#     output, (hn, cn) = self.lstm2(output, (hn, cn)) #lstm with input, hidden, and internal state\n",
    "#     hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "#     out = self.relu(hn)\n",
    "#     out = self.fc_1(out) #first Dense\n",
    "#     out = self.relu(out) #relu\n",
    "#     out = self.fc(out) #Final Output\n",
    "   \n",
    "#     return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30000 #1000 epochs\n",
    "learning_rate = 0.005 #0.001 lr\n",
    "\n",
    "input_size = 24 #number of features\n",
    "hidden_size = 512 #number of features in hidden state\n",
    "\n",
    "num_classes = 1 #number of output classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]).to(device)\n",
    "\n",
    "# loss_function = torch.nn.L1Loss()    # mean-squared error for regression\n",
    "# optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)  # adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     outputs = lstm1.forward(X_train_tensors_final.to(device)) #forward pass\n",
    "#     optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "    \n",
    "#     # obtain the loss function\n",
    "#     loss = loss_function(outputs, y_train_tensors.to(device))\n",
    "\n",
    "#     loss.backward() #calculates the loss of the loss function\n",
    "    \n",
    "#     optimizer.step() #improve from loss, i.e backprop\n",
    "#     if epoch%20 == 0:\n",
    "#         print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()),end='')\n",
    "#         outputs = lstm1.forward(X_test_tensors_final.to(device)) #forward pass\n",
    "#         loss = loss_function(outputs, y_test_tensors.to(device))\n",
    "#         loss.backward() #calculates the loss of the loss function\n",
    "#         print(\"test data loss\",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class simpleLSTM(nn.Module):\n",
    "#   def __init__(self, num_classes, input_size, hidden_size):\n",
    "#     super(simpleLSTM, self).__init__()\n",
    "    \n",
    "#     self.model = nn.Sequential(\n",
    "#     nn.GRU(input_size,hidden_size[0],num_layers=2,batch_first = True,bidirectional=True),\n",
    "#     # nn.Tanh(),/\n",
    "#     # nn.Linear(hidden_size[0]*2,hidden_size[1]),\n",
    "#     nn.ReLU(),\n",
    "#     nn.GRU(hidden_size[1],hidden_size[1],num_layers=1,batch_first = True,bidirectional=True),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(hidden_size[1]*2,3072),\n",
    "#     nn.ReLU(),\n",
    "#     )\n",
    "#     self.out = nn.Linear(3072,num_classes)\n",
    "\n",
    "#   def forward(self,x):\n",
    "#     y =  self.model(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,bidirectional =True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # input x : (BATCH, LENGTH, INPUT_SIZE) 입니다 (다양한 length를 다룰 수 있습니다.).\n",
    "        # 최초의 hidden state와 cell state를 초기화시켜주어야 합니다.\n",
    "        # 배치 사이즈는 가변적이므로 클래스 내에선 표현하지 않습니다.\n",
    "        # 만약 Bi-directional LSTM이라면 아래의 hidden and cell states의 첫번째 차원은 2*self.num_layers 입니다. \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # (BATCH SIZE, SEQ_LENGTH, HIDDEN_SIZE)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # hidden state와 동일\n",
    "\n",
    "        # LSTM 순전파\n",
    "        out, _ = self.lstm(x, (h0, c0)) # output : (BATCH_SIZE, SEQ_LENGTH, HIDDEN_SIZE) tensors. (hn, cn)은 필요 없으므로 받지 않고 _로 처리합니다. \n",
    "\n",
    "        # 마지막 time step(sequence length)의 hidden state를 사용해 Class들의 logit을 반환합니다(hidden_size -> num_classes). \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LSTM1.__init__() missing 1 required positional argument: 'seq_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ADMIN\\Documents\\vsc\\teamteam\\2023_1_KMU_TeamTeamClass\\pytorchLSTM.ipynb Cell 21\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m simpleLSTM \u001b[39m=\u001b[39m LSTM1(input_size, hidden_size,\u001b[39m3\u001b[39;49m,num_classes)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_loss_function \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()    \u001b[39m# mean-squared error for regression\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_loss_function \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mL1Loss()\n",
      "\u001b[1;31mTypeError\u001b[0m: LSTM1.__init__() missing 1 required positional argument: 'seq_length'"
     ]
    }
   ],
   "source": [
    "simpleLSTM = LSTM1(input_size, hidden_size,3,num_classes).to(device)\n",
    "\n",
    "train_loss_function = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "test_loss_function = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(simpleLSTM.parameters(), lr=learning_rate)  # adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (6, 17519, 512), got [3, 17519, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ADMIN\\Documents\\vsc\\teamteam\\2023_1_KMU_TeamTeamClass\\pytorchLSTM.ipynb Cell 22\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     outputs \u001b[39m=\u001b[39m simpleLSTM\u001b[39m.\u001b[39;49mforward(X_train_tensors_final\u001b[39m.\u001b[39;49mto(device)) \u001b[39m#forward pass\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_loss_function(outputs, y_train_tensors\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch\u001b[39m%\u001b[39m\u001b[39m20\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\Documents\\vsc\\teamteam\\2023_1_KMU_TeamTeamClass\\pytorchLSTM.ipynb Cell 22\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(device) \u001b[39m# hidden state와 동일\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# LSTM 순전파\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0)) \u001b[39m# output : (BATCH_SIZE, SEQ_LENGTH, HIDDEN_SIZE) tensors. (hn, cn)은 필요 없으므로 받지 않고 _로 처리합니다. \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# 마지막 time step(sequence length)의 hidden state를 사용해 Class들의 logit을 반환합니다(hidden_size -> num_classes). \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Documents/vsc/teamteam/2023_1_KMU_TeamTeamClass/pytorchLSTM.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:772\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    769\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 772\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:698\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    693\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    694\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    695\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    696\u001b[0m                        ):\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[1;32m--> 698\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_hidden_size(hidden[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_expected_hidden_size(\u001b[39minput\u001b[39;49m, batch_sizes),\n\u001b[0;32m    699\u001b[0m                            \u001b[39m'\u001b[39;49m\u001b[39mExpected hidden[0] size \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m, got \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    701\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:231\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_hidden_size\u001b[39m(\u001b[39mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m],\n\u001b[0;32m    229\u001b[0m                       msg: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mExpected hidden size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 231\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(expected_hidden_size, \u001b[39mlist\u001b[39m(hx\u001b[39m.\u001b[39msize())))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (6, 17519, 512), got [3, 17519, 512]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    outputs = simpleLSTM.forward(X_train_tensors_final.to(device)) #forward pass\n",
    "    loss = train_loss_function(outputs, y_train_tensors.to(device))\n",
    "\n",
    "    if epoch%20 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()),end=', ')\n",
    "        outputs = simpleLSTM.forward(X_test_tensors_final.to(device)) #forward pass\n",
    "        test_loss = test_loss_function(outputs, y_test_tensors.to(device))\n",
    "        # loss.backward() #calculates the loss of the loss function\n",
    "\n",
    "        print(\"test data loss\",test_loss.item())\n",
    "\n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
