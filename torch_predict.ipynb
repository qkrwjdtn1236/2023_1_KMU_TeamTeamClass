{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import data_prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        # chomp_size(padding size): (kernel_size-1) * dilation_size \n",
    "        self.chomp_size = chomp_size  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # output: (N, C_in, L_in - chomp_size)\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        #--------------------- Dilated Causal Convolution --------------------- \n",
    "        '''\n",
    "        input: (N, C_in, L_in) -> (N, C_out, L_in)\n",
    "        output sequence의 길이는 변하지 않는다. \n",
    "        '''\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        #----------------------------------------------------------------------\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        #--------------------- Dilated Causal Convolution --------------------- \n",
    "        '''\n",
    "        input: (N, C_in, L_in) -> output: (N, C_out, L_in)\n",
    "        output sequence의 길이는 변하지 않는다. \n",
    "        '''\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        #----------------------------------------------------------------------\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Dilated Causal Conv -> WeightedNorm -> ReLU -> Dropout -> ... (논문의 Residual block 구조와 동일)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        # Residual Connections \n",
    "        '''  \n",
    "        만약 Dilated Causal Conv를 적용하기 전의 input channel과 적용한 후의 \n",
    "        output channel이 달라질 경우를 대비하여 추가적인 1*1 convolution을 추가해줌.\n",
    "\n",
    "        input: (N, C_in, L_in) -> output: (N, C_out, L_in)\n",
    "        output sequence의 길이는 변하지 않는다. (1*1 convolution)\n",
    "        '''  \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            # dilation factor는 layer의 깊이에 지수적으로 증가 (2^i, i: layer depth) \n",
    "            dilation_size = 2 ** i \n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            # 설정된 layer의 깊이만큼 TemporalBlock 생성 \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs have to have dimension (N, C_in, L_in)\n",
    "        Flatten된 MNIST의 경우에는 (batch_size, 1, 784)\n",
    "        \"\"\"\n",
    "        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n",
    "        o = self.linear(y1[:, :, -1]) # sequence의 마지막 time step로 linear계산 -> 분류예측\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300 #1000 epochs\n",
    "learning_rate = 0.01 #0.001 lr\n",
    "\n",
    "kernel_size = 5\n",
    "stride = 1\n",
    "dilation = 1\n",
    "padding=2\n",
    "dropout=0.2\n",
    "input_size = 24 #number of features\n",
    "hidden_size = 256 #number of features in hidden state\n",
    "num_channels = [512,512,256] #number of stacked lstm layers\n",
    "\n",
    "output_size = 1 #number of output classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = TCN(input_size, output_size, num_channels, kernel_size, dropout)\n",
    "model.load_state_dict(torch.load('tcn_600epochs.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = data_prepro.loadData('datasets/data_tr_city.csv','datasets/data_ts_city.csv',trainYearRange1=2019,trainYearRange2=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = data_prepro.XDataToXAndYSeq(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24, 1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-24:].reshape(1,24,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(365*24):\n",
    "    pred = model(torch.Tensor(X[-24:]).reshape(1,24,1)).tolist()[0][0]\n",
    "    X.append(pred)\n",
    "    Y_pred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04986804351210594,\n",
       " -0.044238969683647156,\n",
       " -0.047725457698106766,\n",
       " -0.04425707459449768,\n",
       " -0.04495525732636452,\n",
       " -0.04756781831383705,\n",
       " -0.046754173934459686,\n",
       " -0.05200258269906044,\n",
       " -0.03776133805513382,\n",
       " -0.04676598310470581,\n",
       " -0.046376731246709824,\n",
       " -0.0525997020304203,\n",
       " -0.04966050013899803,\n",
       " -0.04052700847387314,\n",
       " -0.04829110950231552,\n",
       " -0.04758134111762047,\n",
       " -0.04097092151641846,\n",
       " -0.04878951236605644,\n",
       " -0.04737686738371849,\n",
       " -0.046871695667505264,\n",
       " -0.0457451269030571,\n",
       " -0.04775553196668625,\n",
       " -0.04460683837532997,\n",
       " -0.04405198618769646]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8424 entries, 0 to 8423\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   datetime             8424 non-null   datetime64[ns]\n",
      " 1   구미 혁신도시배수지 유출유량 적산차  8424 non-null   float64       \n",
      " 2   year                 8424 non-null   int64         \n",
      " 3   month                8424 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2)\n",
      "memory usage: 263.4 KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
